{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from itertools import chain\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AmazonPreprocessor:\n",
    "    def __init__(self, train_jpeg_dir, train_csv_file, test_jpeg_dir, test_additional_jpeg_dir,\n",
    "                 img_resize=(32, 32), validation_split=0.2, process_count=cpu_count()):\n",
    "        \"\"\"\n",
    "        This class is used by the classifier to preprocess certains data, don't forget to call the init() method\n",
    "        after an object from this class gets created\n",
    "        :param validation_split: float\n",
    "            Value between 0 and 1 used to split training set from validation set\n",
    "        :param train_jpeg_dir: string\n",
    "            The directory of the train files\n",
    "        :param train_csv_file: string\n",
    "            The path of the file containing the training labels\n",
    "        :param test_jpeg_dir: string\n",
    "            The directory of the all the test images\n",
    "        :param test_additional_jpeg_dir: string\n",
    "            The directory of the all the additional test images\n",
    "        :param img_resize: tuple (int, int)\n",
    "            The resize size of the original image given by the file_path argument\n",
    "        :param process_count: int\n",
    "            The number of process you want to use to preprocess the data.\n",
    "            If you run into issues, lower this number. Its default value is equal to the number of core of your CPU\n",
    "        \"\"\"\n",
    "        self.process_count = process_count\n",
    "        self.validation_split = validation_split\n",
    "        self.img_resize = img_resize\n",
    "        self.test_additional_jpeg_dir = test_additional_jpeg_dir\n",
    "        self.test_jpeg_dir = test_jpeg_dir\n",
    "        self.train_csv_file = train_csv_file\n",
    "        self.train_jpeg_dir = train_jpeg_dir\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.y_map = None\n",
    "        self.X_test = None\n",
    "        self.X_test_filename = None\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor and preprocess required data for the classifier to use\n",
    "        \"\"\"\n",
    "        self.X_train, self.y_train, self.X_val, self.y_val, self.y_map = self._get_train_data_files()\n",
    "        # Contains all the test files including the additional ones\n",
    "        self.X_test, self.X_test_filename = self._get_test_data_files()\n",
    "\n",
    "        if not self.img_resize:\n",
    "            self.img_resize = Image.open(self.X_test_filename[0]).size\n",
    "            print(\"Default image size is\", self.img_resize)\n",
    "\n",
    "        # The validation data cannot be preprocessed in batches as we also need them to compute the f2 score\n",
    "        self.X_val, self.y_val = self._preprocess_val_files()\n",
    "\n",
    "    def get_train_generator(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns a batch generator which transforms chunk of raw images into numpy matrices\n",
    "        and then \"yield\" them for the classifier. Doing so allow to greatly optimize\n",
    "        memory usage as the images are processed then deleted by chunks (defined by batch_size)\n",
    "        instead of preprocessing them all at once and feeding them to the classifier.\n",
    "        :param batch_size: int\n",
    "            The batch size\n",
    "        :return: generator\n",
    "            The batch generator\n",
    "        \"\"\"\n",
    "        # Image Augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True)  # randomly flip images horizontally\n",
    "        loop_range = len(self.X_train)\n",
    "        while True:\n",
    "            for i in range(loop_range):\n",
    "                start_offset = batch_size * i\n",
    "\n",
    "                # The last remaining files could be smaller than the batch_size\n",
    "                range_offset = min(batch_size, loop_range - start_offset)\n",
    "\n",
    "                # If we reached the end of the list then we break the loop\n",
    "                if range_offset <= 0:\n",
    "                    break\n",
    "\n",
    "                batch_features = np.zeros((range_offset, *self.img_resize, 3))\n",
    "                batch_labels = np.zeros((range_offset, len(self.y_train[0])))\n",
    "\n",
    "                for j in range(range_offset):\n",
    "                    # Maybe shuffle the index?\n",
    "                    img = Image.open(self.X_train[start_offset + j])\n",
    "                    img.thumbnail(self.img_resize)\n",
    "\n",
    "                    # Augment the image `img` here\n",
    "\n",
    "                    # Convert to RGB and normalize\n",
    "                    img_array = np.asarray(img.convert(\"RGB\"), dtype=np.float32)\n",
    "\n",
    "                    img_array = img_array[:, :, ::-1]\n",
    "                    # Zero-center by mean pixel\n",
    "                    img_array[:, :, 0] -= 103.939\n",
    "                    img_array[:, :, 1] -= 116.779\n",
    "                    img_array[:, :, 2] -= 123.68\n",
    "\n",
    "                    batch_features[j] = img_array\n",
    "                    batch_labels[j] = self.y_train[start_offset + j]\n",
    "\n",
    "                # Augment the images (using Keras allow us to add randomization/shuffle to augmented images)\n",
    "                # Here the next batch of the data generator (and only one for this iteration)\n",
    "                # is taken and returned in the yield statement\n",
    "                yield next(datagen.flow(batch_features, batch_labels, range_offset))\n",
    "\n",
    "    def get_prediction_generator(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns a batch generator which transforms chunk of raw images into numpy matrices\n",
    "        and then \"yield\" them for the classifier. Doing so allow to greatly optimize\n",
    "        memory usage as the images are processed then deleted by chunks (defined by batch_size)\n",
    "        instead of preprocessing them all at once and feeding them to the classifier.\n",
    "        :param batch_size: int\n",
    "            The batch size\n",
    "        :return: generator\n",
    "            The batch generator\n",
    "        \"\"\"\n",
    "\n",
    "        # NO SHUFFLE HERE as we need our predictions to be in the same order as the inputs\n",
    "        loop_range = len(self.X_test_filename)\n",
    "        while True:\n",
    "            for i in range(loop_range):\n",
    "                start_offset = batch_size * i\n",
    "\n",
    "                # The last remaining files could be smaller than the batch_size\n",
    "                range_offset = min(batch_size, loop_range - start_offset)\n",
    "\n",
    "                # If we reached the end of the list then we break the loop\n",
    "                if range_offset <= 0:\n",
    "                    break\n",
    "\n",
    "                img_arrays = np.zeros((range_offset, *self.img_resize, 3))\n",
    "\n",
    "                for j in range(range_offset):\n",
    "                    img = Image.open(self.X_test_filename[start_offset + j])\n",
    "                    img.thumbnail(self.img_resize)\n",
    "\n",
    "                    # Convert to RGB and normalize\n",
    "                    img_array = np.asarray(img.convert(\"RGB\"), dtype=np.float32)\n",
    "\n",
    "                    img_array = img_array[:, :, ::-1]\n",
    "                    # Zero-center by mean pixel\n",
    "                    img_array[:, :, 0] -= 103.939\n",
    "                    img_array[:, :, 1] -= 116.779\n",
    "                    img_array[:, :, 2] -= 123.68\n",
    "                    img_array = img_array / 255\n",
    "\n",
    "                    img_arrays[j] = img_array\n",
    "                yield img_arrays\n",
    "\n",
    "    def _get_class_mapping(self, *args):\n",
    "        \"\"\"\n",
    "        :param args: list of arguments\n",
    "            file_path: string\n",
    "                The path of the image\n",
    "            tags_str: string\n",
    "                The associated tags as 1 string\n",
    "            labels_map: dict {int: string}\n",
    "                The map between the image label and their id\n",
    "        :return: img_array, targets\n",
    "            file_path: string\n",
    "                The path to the file\n",
    "            targets: Numpy array\n",
    "                A 17 length vector\n",
    "        \"\"\"\n",
    "        # Unpack the *args\n",
    "        file_path, tags_str, labels_map = list(args[0])\n",
    "        targets = np.zeros(len(labels_map))\n",
    "\n",
    "        for t in tags_str.split(' '):\n",
    "            targets[labels_map[t]] = 1\n",
    "        return file_path, targets\n",
    "\n",
    "    def _get_validation_split(self):\n",
    "        train = pd.read_csv(self.train_csv_file)\n",
    "        # mapping labels to integer classes\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        labels = list(set(flatten([l.split(' ') for l in train['tags'].values])))\n",
    "        label_map = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "        y_train = []\n",
    "        for f,tags in (train.values):\n",
    "            targets = np.zeros(len(label_map))\n",
    "            for t in tags.split(' '):\n",
    "                targets[label_map[t]] = 1\n",
    "            y_train.append(targets)\n",
    "\n",
    "        y_train = np.array(y_train, np.uint8)\n",
    "        trn_index = []\n",
    "        val_index = []\n",
    "        index = np.arange(len(train))\n",
    "        for i in (range(len(label_map))):\n",
    "            sss = StratifiedShuffleSplit(n_splits=2, test_size=self.validation_split, random_state=i)\n",
    "            for train_index, test_index in sss.split(index,y_train[:,i]):\n",
    "                X_train, X_test = index[train_index], index[test_index]\n",
    "            # to ensure there is no repetetion within each split and between the splits\n",
    "            trn_index = trn_index + list(set(X_train) - set(trn_index) - set(val_index))\n",
    "            val_index = val_index + list(set(X_test) - set(val_index) - set(trn_index))\n",
    "        return np.array(trn_index), np.array(val_index)\n",
    "\n",
    "    def _get_train_data_files(self):\n",
    "        labels_df = pd.read_csv(self.train_csv_file)\n",
    "        x_train_files, y_train_files = [], []\n",
    "        x_val_files, y_val_files = [], []\n",
    "        train_files, train_tags = [], []\n",
    "        val_files, val_tags = [], []\n",
    "\n",
    "        files_path = []\n",
    "        tags_list = []\n",
    "        for file_name, tags in labels_df.values:\n",
    "            files_path.append('{}/{}.jpg'.format(self.train_jpeg_dir, file_name))\n",
    "            tags_list.append(tags)\n",
    "\n",
    "\n",
    "        if self.validation_split != 0:\n",
    "\n",
    "            trn_index, val_index = self._get_validation_split()\n",
    "            for index in trn_index:\n",
    "                train_files.append(files_path[index])\n",
    "                train_tags.append(tags_list[index])\n",
    "            for index in val_index:\n",
    "                val_files.append(files_path[index])\n",
    "                val_tags.append(tags_list[index])\n",
    "\n",
    "        else:\n",
    "            train_files = files_path\n",
    "            train_tags = tags_list\n",
    "\n",
    "\n",
    "        labels = sorted(set(chain.from_iterable([tags.split(\" \") for tags in labels_df['tags'].values])))\n",
    "        y_map = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "        with ThreadPoolExecutor(self.process_count) as pool:\n",
    "            for file_name, targets in tqdm(pool.map(self._get_class_mapping,\n",
    "                                                    [(file_name, tags, y_map)\n",
    "                                                     for file_name, tags in zip(train_files, train_tags)]),\n",
    "                                           total=len(train_files)):\n",
    "                x_train_files.append(file_name)\n",
    "                y_train_files.append(targets)\n",
    "\n",
    "        if self.validation_split != 0:\n",
    "            with ThreadPoolExecutor(self.process_count) as pool:\n",
    "                for file_name, targets in tqdm(pool.map(self._get_class_mapping,\n",
    "                                                        [(file_name, tags, y_map)\n",
    "                                                         for file_name, tags in zip(val_files, val_tags)]),\n",
    "                                               total=len(val_files)):\n",
    "                    x_val_files.append(file_name)\n",
    "                    y_val_files.append(targets)\n",
    "\n",
    "        return [x_train_files, y_train_files, x_val_files, y_val_files, {v: k for k, v in y_map.items()}]\n",
    "\n",
    "    def _val_transform_to_matrices(self, *args):\n",
    "        \"\"\"\n",
    "        :param args: list of arguments\n",
    "            file_name: string\n",
    "                The name of the image\n",
    "            :return: img_array, file_name\n",
    "                img_array: Numpy array\n",
    "                    The image from the file_path as a numpy array resized with img_resize\n",
    "                file_name: string\n",
    "                    The name of the test image\n",
    "            \"\"\"\n",
    "        file_path, val_labels = list(args[0])\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail(self.img_resize)\n",
    "\n",
    "        # Augment the image `img` here\n",
    "\n",
    "        # Convert to RGB and normalize\n",
    "        img_array = np.array(img.convert(\"RGB\"), dtype=np.float32)\n",
    "        img_array = img_array[:, :, ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        img_array[:, :, 0] -= 103.939\n",
    "        img_array[:, :, 1] -= 116.779\n",
    "        img_array[:, :, 2] -= 123.68\n",
    "        img_array = img_array / 255\n",
    "\n",
    "        return img_array, val_labels\n",
    "\n",
    "    def _preprocess_val_files(self):\n",
    "        \"\"\"\n",
    "        Transform the images to ready to use data for the CNN\n",
    "        :param val_labels: list\n",
    "            List of file labels\n",
    "        :param val_files: list\n",
    "            List of file path\n",
    "        :param process_count: int\n",
    "            The number of process you want to use to preprocess the data.\n",
    "            If you run into issues, lower this number. Its default value is equal to the number of core of your CPU\n",
    "        :return: The images matrices and labels as [x_test, x_test_filename]\n",
    "            x_test: The X test values as a numpy array\n",
    "            x_test_filename: The files name of each test images in the same order as the x_test arrays\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        final_val_labels = []\n",
    "\n",
    "        # Multiprocess transformation, the map() function take a function as a 1st argument\n",
    "        # and the argument to pass to it as the 2nd argument. These arguments are processed\n",
    "        # asynchronously on threads defined by process_count and their results are stored in\n",
    "        # the x_test and x_test_filename lists\n",
    "        print(\"Transforming val dataset...\")\n",
    "        sys.stdout.flush()\n",
    "        with ThreadPoolExecutor(self.process_count) as pool:\n",
    "            for img_array, targets in tqdm(pool.map(self._val_transform_to_matrices,\n",
    "                                                    [(file_path, labels)\n",
    "                                                     for file_path, labels in zip(self.X_val, self.y_val)]),\n",
    "                                           total=len(self.X_val)):\n",
    "                x.append(img_array)\n",
    "                final_val_labels.append(targets)\n",
    "        ret = [np.array(x), np.array(final_val_labels)]\n",
    "        print(\"Done. Size consumed by validation matrices {} mb\".format(ret[0].nbytes / 1024 / 1024))\n",
    "        sys.stdout.flush()\n",
    "        return ret\n",
    "\n",
    "    def _get_test_data_files(self):\n",
    "        files_name = os.listdir(self.test_jpeg_dir)\n",
    "        files_name_add = os.listdir(self.test_additional_jpeg_dir)\n",
    "        # ! hstack is deprecated\n",
    "        X_test_filename = np.hstack(([name.split(\".\")[0] for name in files_name],\n",
    "                                     [name.split(\".\")[0] for name in files_name_add]))\n",
    "        X_test_file_path = np.hstack(([self.test_jpeg_dir + \"/\" + name for name in files_name],\n",
    "                                      [self.test_additional_jpeg_dir + \"/\" + name for name in files_name_add]))\n",
    "        return X_test_filename, X_test_file_path\n",
    "\n",
    "\n",
    "def get_jpeg_data_files_paths():\n",
    "    \"\"\"\n",
    "    Returns the input file folders path\n",
    "    :return: list of strings\n",
    "        The input file paths as list [train_jpeg_dir, test_jpeg_dir, test_jpeg_additional, train_csv_file]\n",
    "    \"\"\"\n",
    "\n",
    "    data_root_folder = os.path.abspath(\"../input/\")\n",
    "    train_jpeg_dir = os.path.join(data_root_folder, 'train-jpg')\n",
    "    test_jpeg_dir = os.path.join(data_root_folder, 'test-jpg')\n",
    "    test_jpeg_additional = os.path.join(data_root_folder, 'test-jpg-additional')\n",
    "    train_csv_file = os.path.join(data_root_folder, 'train_v2.csv')\n",
    "    return [train_jpeg_dir, test_jpeg_dir, test_jpeg_additional, train_csv_file]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
